import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import json
import pickle
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import os
import time
import pandas as pd
from datetime import datetime

# ê¸°ì¡´ í´ë˜ìŠ¤ë“¤ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (PolynomialDataset, PolynomialRootNet ë“±)
class PolynomialDataset(Dataset):
    def __init__(self, data_file, train=True, train_ratio=0.8, normalize=True):
        """
        ì €ì¥ëœ ë°ì´í„° íŒŒì¼ë¡œë¶€í„° Dataset ìƒì„±
        
        Args:
            data_file: ë°ì´í„° íŒŒì¼ ê²½ë¡œ (.json ë˜ëŠ” .pkl)
            train: Trueë©´ í›ˆë ¨ìš©, Falseë©´ í…ŒìŠ¤íŠ¸ìš©
            train_ratio: í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í•  ë¹„ìœ¨
            normalize: ë°ì´í„° ì •ê·œí™” ì—¬ë¶€
        """
        self.data = self.load_data(data_file)
        self.normalize = normalize
        self.coeffs, self.roots = self.prepare_data()
        
        # ì •ê·œí™” íŒŒë¼ë¯¸í„° ì €ì¥
        if normalize:
            self.coeff_mean = np.mean(self.coeffs, axis=0)
            self.coeff_std = np.std(self.coeffs, axis=0) + 1e-8  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
            self.root_mean = np.mean(self.roots, axis=0)
            self.root_std = np.std(self.roots, axis=0) + 1e-8
            
            # ì •ê·œí™” ì ìš©
            self.coeffs = (self.coeffs - self.coeff_mean) / self.coeff_std
            self.roots = (self.roots - self.root_mean) / self.root_std
        
        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
        train_coeffs, test_coeffs, train_roots, test_roots = train_test_split(
            self.coeffs, self.roots, train_size=train_ratio, random_state=42
        )
        
        if train:
            self.coeffs = train_coeffs
            self.roots = train_roots
        else:
            self.coeffs = test_coeffs
            self.roots = test_roots
        
        print(f"{'í›ˆë ¨' if train else 'í…ŒìŠ¤íŠ¸'} ë°ì´í„°ì…‹: {len(self.coeffs)}ê°œ")
        if normalize:
            print(f"ë°ì´í„° ì •ê·œí™” ì ìš©ë¨")
    
    def load_data(self, filename):
        """ë°ì´í„° íŒŒì¼ ë¡œë“œ"""
        if filename.endswith('.json'):
            with open(filename, 'r', encoding='utf-8') as f:
                return json.load(f)
        elif filename.endswith('.pkl'):
            with open(filename, 'rb') as f:
                return pickle.load(f)
        else:
            raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. (.json ë˜ëŠ” .pkl ì‚¬ìš©)")
    
    def prepare_data(self):
        """ë°ì´í„°ë¥¼ í•™ìŠµìš© í˜•íƒœë¡œ ë³€í™˜"""
        coeffs = []
        roots = []
        
        for item in self.data:
            coeffs.append(item['coefficients'])
            roots.append(item['roots'])
        
        return np.array(coeffs, dtype=np.float32), np.array(roots, dtype=np.float32)
    
    def denormalize_roots(self, normalized_roots):
        """ê·¼ì„ ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë˜ëŒë¦¬ê¸°"""
        if self.normalize and hasattr(self, 'root_mean'):
            return normalized_roots * self.root_std + self.root_mean
        return normalized_roots
    
    def __len__(self):
        return len(self.coeffs)
    
    def __getitem__(self, idx):
        return torch.FloatTensor(self.coeffs[idx]), torch.FloatTensor(self.roots[idx])

class PolynomialRootNet(nn.Module):
    def __init__(self, input_size=6, output_size=10, hidden_size=512, num_layers=4, dropout=0.3):
        super(PolynomialRootNet, self).__init__()
        
        layers = []
        current_size = input_size
        
        # ì²« ë²ˆì§¸ ì¸µ
        layers.extend([
            nn.Linear(current_size, hidden_size),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_size),
            nn.Dropout(dropout)
        ])
        current_size = hidden_size
        
        # ì¤‘ê°„ ì¸µë“¤
        for i in range(num_layers - 2):
            next_size = hidden_size // (2 ** (i + 1))
            next_size = max(next_size, 64)  # ìµœì†Œ 64ê°œ ë‰´ëŸ°
            
            layers.extend([
                nn.Linear(current_size, next_size),
                nn.ReLU(),
                nn.BatchNorm1d(next_size),
                nn.Dropout(dropout * 0.8)  # ì ì§„ì ìœ¼ë¡œ ë“œë¡­ì•„ì›ƒ ê°ì†Œ
            ])
            current_size = next_size
        
        # ì¶œë ¥ì¸µ
        layers.append(nn.Linear(current_size, output_size))
        
        self.network = nn.Sequential(*layers)
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        return self.network(x)

class HiddenSizeComparator:
    def __init__(self, data_file, device='cpu'):
        self.data_file = data_file
        self.device = device
        self.results = []
        
        print(f"ë””ë°”ì´ìŠ¤: {device}")
        print(f"ë°ì´í„° íŒŒì¼: {data_file}")
        
    def create_datasets(self, batch_size=32):
        """ë°ì´í„°ì…‹ ìƒì„±"""
        train_dataset = PolynomialDataset(self.data_file, train=True, normalize=True)
        test_dataset = PolynomialDataset(self.data_file, train=False, normalize=True)
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        return train_loader, test_loader, test_dataset
    
    def train_single_model(self, hidden_size, num_layers=4, epochs=50, lr=0.001, batch_size=32, dropout=0.3):
        """ë‹¨ì¼ ëª¨ë¸ í›ˆë ¨"""
        print(f"\n{'='*50}")
        print(f"Hidden Size: {hidden_size}, Layers: {num_layers}")
        print(f"{'='*50}")
        
        # ë°ì´í„° ë¡œë” ìƒì„±
        train_loader, test_loader, test_dataset = self.create_datasets(batch_size)
        
        # ëª¨ë¸ ìƒì„±
        model = PolynomialRootNet(
            hidden_size=hidden_size, 
            num_layers=num_layers, 
            dropout=dropout
        ).to(self.device)
        
        # ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}")
        print(f"í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜: {trainable_params:,}")
        
        # í›ˆë ¨ ì„¤ì •
        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)
        criterion = nn.MSELoss()
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=10
        )
        
        # í›ˆë ¨ ê¸°ë¡
        train_losses = []
        test_losses = []
        best_test_loss = float('inf')
        best_model_state = None
        
        # í›ˆë ¨ ì‹œì‘ ì‹œê°„
        start_time = time.time()
        
        # í›ˆë ¨ ë£¨í”„
        for epoch in range(epochs):
            # í›ˆë ¨
            model.train()
            train_loss = 0
            num_batches = 0
            
            for coeffs, roots in train_loader:
                coeffs, roots = coeffs.to(self.device), roots.to(self.device)
                
                optimizer.zero_grad()
                pred_roots = model(coeffs)
                loss = criterion(pred_roots, roots)
                loss.backward()
                
                # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()
                
                train_loss += loss.item()
                num_batches += 1
            
            train_loss /= num_batches
            train_losses.append(train_loss)
            
            # í…ŒìŠ¤íŠ¸
            model.eval()
            test_loss = 0
            num_batches = 0
            
            with torch.no_grad():
                for coeffs, roots in test_loader:
                    coeffs, roots = coeffs.to(self.device), roots.to(self.device)
                    pred_roots = model(coeffs)
                    loss = criterion(pred_roots, roots)
                    test_loss += loss.item()
                    num_batches += 1
            
            test_loss /= num_batches
            test_losses.append(test_loss)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            scheduler.step(test_loss)
            
            # ìµœê³  ëª¨ë¸ ì €ì¥
            if test_loss < best_test_loss:
                best_test_loss = test_loss
                best_model_state = model.state_dict().copy()
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if epoch % 10 == 0 or epoch == epochs - 1:
                print(f"Epoch {epoch:3d}: Train={train_loss:.6f}, Test={test_loss:.6f}, Best={best_test_loss:.6f}")
        
        # í›ˆë ¨ ì‹œê°„ ê³„ì‚°
        training_time = time.time() - start_time
        
        # ìµœê³  ëª¨ë¸ ë¡œë“œ
        if best_model_state:
            model.load_state_dict(best_model_state)
        
        # ê²°ê³¼ ì €ì¥
        result = {
            'hidden_size': hidden_size,
            'num_layers': num_layers,
            'total_params': total_params,
            'trainable_params': trainable_params,
            'best_test_loss': best_test_loss,
            'final_train_loss': train_losses[-1],
            'final_test_loss': test_losses[-1],
            'training_time': training_time,
            'train_losses': train_losses,
            'test_losses': test_losses,
            'epochs': epochs,
            'learning_rate': lr,
            'batch_size': batch_size,
            'dropout': dropout
        }
        
        self.results.append(result)
        
        print(f"í›ˆë ¨ ì™„ë£Œ - ìµœê³  í…ŒìŠ¤íŠ¸ ì†ì‹¤: {best_test_loss:.6f}")
        print(f"í›ˆë ¨ ì‹œê°„: {training_time:.1f}ì´ˆ")
        
        return model, result
    
    def compare_hidden_sizes(self, hidden_sizes, num_layers=4, epochs=50, lr=0.001, batch_size=32, dropout=0.3):
        """ì—¬ëŸ¬ hidden_sizeë¡œ ë¹„êµ ì‹¤í—˜"""
        print(f"Hidden Size ë¹„êµ ì‹¤í—˜ ì‹œì‘")
        print(f"í…ŒìŠ¤íŠ¸í•  Hidden Sizes: {hidden_sizes}")
        print(f"Epochs: {epochs}, Learning Rate: {lr}")
        
        for hidden_size in hidden_sizes:
            try:
                model, result = self.train_single_model(
                    hidden_size=hidden_size,
                    num_layers=num_layers,
                    epochs=epochs,
                    lr=lr,
                    batch_size=batch_size,
                    dropout=dropout
                )
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del model
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
                
            except Exception as e:
                print(f"Hidden Size {hidden_size} í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue
        
        print(f"\nëª¨ë“  ì‹¤í—˜ ì™„ë£Œ!")
        return self.results
    
    def plot_comparison(self, save_path=None):
        """ë¹„êµ ê²°ê³¼ ì‹œê°í™”"""
        if not self.results:
            print("ë¹„êµí•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        df = pd.DataFrame(self.results)
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Hidden Size ë¹„êµ ì‹¤í—˜ ê²°ê³¼', fontsize=16, fontweight='bold')
        
        # 1. í…ŒìŠ¤íŠ¸ ì†ì‹¤ ë¹„êµ
        axes[0, 0].bar(range(len(df)), df['best_test_loss'], alpha=0.7, color='skyblue')
        axes[0, 0].set_xlabel('Hidden Size')
        axes[0, 0].set_ylabel('Best Test Loss')
        axes[0, 0].set_title('ìµœê³  í…ŒìŠ¤íŠ¸ ì†ì‹¤ ë¹„êµ')
        axes[0, 0].set_xticks(range(len(df)))
        axes[0, 0].set_xticklabels(df['hidden_size'], rotation=45)
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. íŒŒë¼ë¯¸í„° ìˆ˜ vs ì„±ëŠ¥
        axes[0, 1].scatter(df['total_params'], df['best_test_loss'], s=100, alpha=0.7, color='coral')
        for i, row in df.iterrows():
            axes[0, 1].annotate(f"{row['hidden_size']}", 
                              (row['total_params'], row['best_test_loss']),
                              xytext=(5, 5), textcoords='offset points', fontsize=8)
        axes[0, 1].set_xlabel('ì´ íŒŒë¼ë¯¸í„° ìˆ˜')
        axes[0, 1].set_ylabel('Best Test Loss')
        axes[0, 1].set_title('íŒŒë¼ë¯¸í„° ìˆ˜ vs ì„±ëŠ¥')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. í›ˆë ¨ ì‹œê°„ ë¹„êµ
        axes[0, 2].bar(range(len(df)), df['training_time'], alpha=0.7, color='lightgreen')
        axes[0, 2].set_xlabel('Hidden Size')
        axes[0, 2].set_ylabel('Training Time (ì´ˆ)')
        axes[0, 2].set_title('í›ˆë ¨ ì‹œê°„ ë¹„êµ')
        axes[0, 2].set_xticks(range(len(df)))
        axes[0, 2].set_xticklabels(df['hidden_size'], rotation=45)
        axes[0, 2].grid(True, alpha=0.3)
        
        # 4. í›ˆë ¨ ê³¡ì„  (ëª¨ë“  ëª¨ë¸)
        for i, result in enumerate(self.results):
            axes[1, 0].plot(result['test_losses'], 
                          label=f"H={result['hidden_size']}", alpha=0.8)
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Test Loss')
        axes[1, 0].set_title('í…ŒìŠ¤íŠ¸ ì†ì‹¤ ê³¡ì„ ')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        axes[1, 0].set_yscale('log')
        
        # 5. íš¨ìœ¨ì„± ë¹„êµ (ì„±ëŠ¥ / íŒŒë¼ë¯¸í„° ìˆ˜)
        efficiency = df['best_test_loss'] / (df['total_params'] / 1000)  # 1000ê°œë‹¹ ì†ì‹¤
        axes[1, 1].bar(range(len(df)), efficiency, alpha=0.7, color='gold')
        axes[1, 1].set_xlabel('Hidden Size')
        axes[1, 1].set_ylabel('Loss per 1K Parameters')
        axes[1, 1].set_title('íš¨ìœ¨ì„± (ì†ì‹¤/íŒŒë¼ë¯¸í„°)')
        axes[1, 1].set_xticks(range(len(df)))
        axes[1, 1].set_xticklabels(df['hidden_size'], rotation=45)
        axes[1, 1].grid(True, alpha=0.3)
        
        # 6. ìˆ˜ë ´ ì†ë„ ë¹„êµ (10 ì—í¬í¬ì—ì„œì˜ í…ŒìŠ¤íŠ¸ ì†ì‹¤)
        convergence_loss = []
        for result in self.results:
            if len(result['test_losses']) >= 10:
                convergence_loss.append(result['test_losses'][9])  # 10ë²ˆì§¸ ì—í¬í¬
            else:
                convergence_loss.append(result['test_losses'][-1])
        
        axes[1, 2].bar(range(len(df)), convergence_loss, alpha=0.7, color='plum')
        axes[1, 2].set_xlabel('Hidden Size')
        axes[1, 2].set_ylabel('Test Loss at Epoch 10')
        axes[1, 2].set_title('ì´ˆê¸° ìˆ˜ë ´ ì†ë„')
        axes[1, 2].set_xticks(range(len(df)))
        axes[1, 2].set_xticklabels(df['hidden_size'], rotation=45)
        axes[1, 2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ê·¸ë˜í”„ ì €ì¥: {save_path}")
        
        plt.show()
    
    def save_results(self, filepath):
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        # CSVë¡œ ìš”ì•½ ì €ì¥
        df = pd.DataFrame(self.results)
        csv_path = filepath + '_summary.csv'
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"ìš”ì•½ ê²°ê³¼ ì €ì¥: {csv_path}")
        
        # ì „ì²´ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
        json_path = filepath + '_detailed.json'
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        print(f"ìƒì„¸ ê²°ê³¼ ì €ì¥: {json_path}")
        
        return csv_path, json_path
    
    def print_summary(self):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        if not self.results:
            print("ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return
        
        print(f"\n{'='*80}")
        print(f"Hidden Size ë¹„êµ ì‹¤í—˜ ìš”ì•½")
        print(f"{'='*80}")
        
        # ì •ë ¬ (í…ŒìŠ¤íŠ¸ ì†ì‹¤ ê¸°ì¤€)
        sorted_results = sorted(self.results, key=lambda x: x['best_test_loss'])
        
        print(f"{'ìˆœìœ„':<4} {'Hidden':<8} {'íŒŒë¼ë¯¸í„°':<12} {'í…ŒìŠ¤íŠ¸ì†ì‹¤':<12} {'í›ˆë ¨ì‹œê°„':<10} {'íš¨ìœ¨ì„±':<10}")
        print(f"{'-'*70}")
        
        for i, result in enumerate(sorted_results):
            efficiency = result['best_test_loss'] / (result['total_params'] / 1000)
            print(f"{i+1:<4} {result['hidden_size']:<8} {result['total_params']:<12,} "
                  f"{result['best_test_loss']:<12.6f} {result['training_time']:<10.1f} {efficiency:<10.4f}")
        
        # ìµœê³  ì„±ëŠ¥
        best = sorted_results[0]
        print(f"\nğŸ† ìµœê³  ì„±ëŠ¥:")
        print(f"   Hidden Size: {best['hidden_size']}")
        print(f"   í…ŒìŠ¤íŠ¸ ì†ì‹¤: {best['best_test_loss']:.6f}")
        print(f"   íŒŒë¼ë¯¸í„° ìˆ˜: {best['total_params']:,}")
        print(f"   í›ˆë ¨ ì‹œê°„: {best['training_time']:.1f}ì´ˆ")
        
        # ê°€ì¥ íš¨ìœ¨ì ì¸ ëª¨ë¸
        most_efficient = min(sorted_results, key=lambda x: x['best_test_loss'] / (x['total_params'] / 1000))
        print(f"\nâš¡ ê°€ì¥ íš¨ìœ¨ì ì¸ ëª¨ë¸:")
        print(f"   Hidden Size: {most_efficient['hidden_size']}")
        print(f"   íš¨ìœ¨ì„±: {most_efficient['best_test_loss'] / (most_efficient['total_params'] / 1000):.4f}")
        print(f"   í…ŒìŠ¤íŠ¸ ì†ì‹¤: {most_efficient['best_test_loss']:.6f}")

def main():
    # ì„¤ì •
    data_file = "polynomial_dataset_2.json"  # ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # ë°ì´í„° íŒŒì¼ í™•ì¸
    if not os.path.exists(data_file):
        print(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_file}")
        print("ë¨¼ì € ë°ì´í„° ìƒì„±ê¸°ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”!")
        return
    
    # ë¹„êµí•  hidden_size ëª©ë¡
    hidden_sizes = [128*2, 128*3, 128*4, 128*5, 128*6, 128*7, 128*8, 128*9, 128*10, 128*11, 128*12, 128*13]  # ì›í•˜ëŠ” í¬ê¸°ë¡œ ì¡°ì • ê°€ëŠ¥
    
    # ì‹¤í—˜ ì„¤ì •
    experiment_config = {
        'num_layers': 6,
        'epochs': 100,        # ì—í¬í¬ ìˆ˜ ì¡°ì •
        'lr': 0.004,
        'batch_size': 32,
        'dropout': 0.3
    }
    
    print(f"Hidden Size ë¹„êµ ì‹¤í—˜ ì„¤ì •:")
    print(f"Hidden Sizes: {hidden_sizes}")
    print(f"ì‹¤í—˜ ì„¤ì •: {experiment_config}")
    
    # ë¹„êµ ì‹¤í—˜ ì‹¤í–‰
    comparator = HiddenSizeComparator(data_file, device)
    results = comparator.compare_hidden_sizes(hidden_sizes, **experiment_config)
    
    # ê²°ê³¼ ì¶œë ¥
    comparator.print_summary()
    
    # ê²°ê³¼ ì‹œê°í™”
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    plot_path = f"hidden_size_comparison_{timestamp}.png"
    comparator.plot_comparison(save_path=plot_path)
    
    # ê²°ê³¼ ì €ì¥
    result_path = f"hidden_size_experiment_{timestamp}"
    comparator.save_results(result_path)
    
    print(f"\nì‹¤í—˜ ì™„ë£Œ! ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ê°œë³„ ì‹¤í—˜ í•¨ìˆ˜ (íŠ¹ì • ì„¤ì •ë§Œ í…ŒìŠ¤íŠ¸í•˜ê³  ì‹¶ì„ ë•Œ)
def quick_test(hidden_sizes=[256, 512, 1024], epochs=30):
    """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš© í•¨ìˆ˜"""
    data_file = "polynomial_dataset_2.json"
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    if not os.path.exists(data_file):
        print(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_file}")
        return
    
    comparator = HiddenSizeComparator(data_file, device)
    results = comparator.compare_hidden_sizes(
        hidden_sizes=hidden_sizes,
        epochs=epochs,
        lr=0.002
    )
    
    comparator.print_summary()
    comparator.plot_comparison()
    
    return results

if __name__ == "__main__":
    main()
    
    # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ì›í•˜ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
    # quick_test(hidden_sizes=[128, 512, 1024], epochs=20)