{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a237bab-8198-457a-953b-a63a7b32943d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 디바이스: cuda\n",
      "그리드 서치 범위:\n",
      "  hidden_size: [384, 512, 640, 768, 896]\n",
      "  num_layers: [6, 7, 8]\n",
      "  dropout: [0.1]\n",
      "  batch_size: [64, 128, 192, 256, 320]\n",
      "총 조합 수: 75\n",
      "데이터셋 로딩 중...\n",
      "훈련 데이터셋: 8000개\n",
      "데이터 정규화 적용됨\n",
      "테스트 데이터셋: 2000개\n",
      "데이터 정규화 적용됨\n",
      "총 75개의 조합을 테스트합니다.\n",
      "================================================================================\n",
      "\n",
      "[1/75] 테스트 중...\n",
      "Hidden Size: 384, Layers: 6, Dropout: 0.1, Batch Size: 64\n",
      "✓ 완료 - Test Loss: 0.155902, 시간: 44.4초\n",
      "  모델 저장: model_h384_l6_d0.1_b64.pth\n",
      "진행률: 1.3% | 예상 남은 시간: 54.8분\n",
      "\n",
      "[2/75] 테스트 중...\n",
      "Hidden Size: 384, Layers: 6, Dropout: 0.1, Batch Size: 128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "\n",
    "class PolynomialDataset(Dataset):\n",
    "    def __init__(self, data_file, train=True, train_ratio=0.8, normalize=True):\n",
    "        \"\"\"\n",
    "        저장된 데이터 파일로부터 Dataset 생성\n",
    "        \n",
    "        Args:\n",
    "            data_file: 데이터 파일 경로 (.json 또는 .pkl)\n",
    "            train: True면 훈련용, False면 테스트용\n",
    "            train_ratio: 훈련/테스트 분할 비율\n",
    "            normalize: 데이터 정규화 여부\n",
    "        \"\"\"\n",
    "        self.data = self.load_data(data_file)\n",
    "        self.normalize = normalize\n",
    "        self.coeffs, self.roots = self.prepare_data()\n",
    "        \n",
    "        # 정규화 파라미터 저장\n",
    "        if normalize:\n",
    "            self.coeff_mean = np.mean(self.coeffs, axis=0)\n",
    "            self.coeff_std = np.std(self.coeffs, axis=0) + 1e-8\n",
    "            self.root_mean = np.mean(self.roots, axis=0)\n",
    "            self.root_std = np.std(self.roots, axis=0) + 1e-8\n",
    "            \n",
    "            # 정규화 적용\n",
    "            self.coeffs = (self.coeffs - self.coeff_mean) / self.coeff_std\n",
    "            self.roots = (self.roots - self.root_mean) / self.root_std\n",
    "        \n",
    "        # 훈련/테스트 분할\n",
    "        train_coeffs, test_coeffs, train_roots, test_roots = train_test_split(\n",
    "            self.coeffs, self.roots, train_size=train_ratio, random_state=42\n",
    "        )\n",
    "        \n",
    "        if train:\n",
    "            self.coeffs = train_coeffs\n",
    "            self.roots = train_roots\n",
    "        else:\n",
    "            self.coeffs = test_coeffs\n",
    "            self.roots = test_roots\n",
    "        \n",
    "        print(f\"{'훈련' if train else '테스트'} 데이터셋: {len(self.coeffs)}개\")\n",
    "        if normalize:\n",
    "            print(f\"데이터 정규화 적용됨\")\n",
    "    \n",
    "    def load_data(self, filename):\n",
    "        \"\"\"데이터 파일 로드\"\"\"\n",
    "        if filename.endswith('.json'):\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        elif filename.endswith('.pkl'):\n",
    "            with open(filename, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        else:\n",
    "            raise ValueError(\"지원하지 않는 파일 형식입니다. (.json 또는 .pkl 사용)\")\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        \"\"\"데이터를 학습용 형태로 변환\"\"\"\n",
    "        coeffs = []\n",
    "        roots = []\n",
    "        \n",
    "        for item in self.data:\n",
    "            coeffs.append(item['coefficients'])\n",
    "            \n",
    "            # 근 데이터 형식 확인 및 변환\n",
    "            item_roots = item['roots']\n",
    "            \n",
    "            if isinstance(item_roots[0], list):\n",
    "                # [[실수부, 허수부], ...] 형태를 [실수부1, 허수부1, ...] 형태로 평탄화\n",
    "                flattened_roots = []\n",
    "                for root in item_roots:\n",
    "                    flattened_roots.extend(root)\n",
    "                roots.append(flattened_roots)\n",
    "            else:\n",
    "                # 이미 [실수부1, 허수부1, ...] 형태\n",
    "                roots.append(item_roots)\n",
    "        \n",
    "        return np.array(coeffs, dtype=np.float32), np.array(roots, dtype=np.float32)\n",
    "    \n",
    "    def denormalize_roots(self, normalized_roots):\n",
    "        \"\"\"근을 원래 스케일로 되돌리기\"\"\"\n",
    "        if self.normalize and hasattr(self, 'root_mean'):\n",
    "            return normalized_roots * self.root_std + self.root_mean\n",
    "        return normalized_roots\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.coeffs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.FloatTensor(self.coeffs[idx]), torch.FloatTensor(self.roots[idx])\n",
    "\n",
    "class OptimizedPolynomialRootNet(nn.Module):\n",
    "    \"\"\"경량화된 효율적인 다항식 근 찾기 네트워크\"\"\"\n",
    "    def __init__(self, input_size=6, output_size=10, hidden_size=512, num_layers=4, dropout=0.3):\n",
    "        super(OptimizedPolynomialRootNet, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        current_size = input_size\n",
    "        \n",
    "        # 첫 번째 층\n",
    "        layers.extend([\n",
    "            nn.Linear(current_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Dropout(dropout)\n",
    "        ])\n",
    "        current_size = hidden_size\n",
    "        \n",
    "        # 중간 층들 (점진적 크기 감소)\n",
    "        for i in range(num_layers - 2):\n",
    "            next_size = hidden_size // (2 ** (i + 1))\n",
    "            next_size = max(next_size, 64)  # 최소 64개 뉴런\n",
    "            \n",
    "            layers.extend([\n",
    "                nn.Linear(current_size, next_size),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(next_size),\n",
    "                nn.Dropout(dropout * 0.8)  # 점진적으로 드롭아웃 감소\n",
    "            ])\n",
    "            current_size = next_size\n",
    "        \n",
    "        # 출력층\n",
    "        layers.append(nn.Linear(current_size, output_size))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class GridSearchTrainer:\n",
    "    def __init__(self, data_file, device='cpu'):\n",
    "        self.data_file = data_file\n",
    "        self.device = device\n",
    "        self.results = []\n",
    "        \n",
    "        # 데이터셋 한 번만 로드 (매번 새로 로드하면 시간 소요)\n",
    "        print(\"데이터셋 로딩 중...\")\n",
    "        self.train_dataset = PolynomialDataset(data_file, train=True, normalize=True)\n",
    "        self.test_dataset = PolynomialDataset(data_file, train=False, normalize=True)\n",
    "        \n",
    "    def complex_aware_loss(self, pred_roots, true_roots, alpha=0.3):\n",
    "        \"\"\"복소수 근을 고려한 손실 함수\"\"\"\n",
    "        mse_loss = nn.MSELoss()(pred_roots, true_roots)\n",
    "        \n",
    "        pred_real = pred_roots[:, 0::2]\n",
    "        pred_imag = pred_roots[:, 1::2]\n",
    "        true_real = true_roots[:, 0::2]\n",
    "        true_imag = true_roots[:, 1::2]\n",
    "        \n",
    "        pred_magnitude = torch.sqrt(pred_real**2 + pred_imag**2)\n",
    "        true_magnitude = torch.sqrt(true_real**2 + true_imag**2)\n",
    "        magnitude_loss = nn.MSELoss()(pred_magnitude, true_magnitude)\n",
    "        \n",
    "        return (1 - alpha) * mse_loss + alpha * magnitude_loss\n",
    "    \n",
    "    def train_single_config(self, hidden_size, num_layers, dropout, batch_size, epochs=80, lr=0.008):\n",
    "        \"\"\"단일 구성으로 모델 훈련\"\"\"\n",
    "        \n",
    "        # 데이터 로더 생성\n",
    "        train_loader = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        test_loader = DataLoader(self.test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "        \n",
    "        # 모델 생성\n",
    "        model = OptimizedPolynomialRootNet(\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        ).to(self.device)\n",
    "        \n",
    "        param_count = sum(p.numel() for p in model.parameters())\n",
    "        \n",
    "        # 옵티마이저 및 스케줄러\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=10\n",
    "        )\n",
    "        \n",
    "        # 훈련 기록\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        best_test_loss = float('inf')\n",
    "        \n",
    "        # 훈련 루프\n",
    "        for epoch in range(epochs):\n",
    "            # 훈련\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for coeffs, roots in train_loader:\n",
    "                coeffs, roots = coeffs.to(self.device), roots.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                pred_roots = model(coeffs)\n",
    "                loss = self.complex_aware_loss(pred_roots, roots)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            train_losses.append(train_loss)\n",
    "            \n",
    "            # 테스트\n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for coeffs, roots in test_loader:\n",
    "                    coeffs, roots = coeffs.to(self.device), roots.to(self.device)\n",
    "                    pred_roots = model(coeffs)\n",
    "                    loss = self.complex_aware_loss(pred_roots, roots)\n",
    "                    test_loss += loss.item()\n",
    "            \n",
    "            test_loss /= len(test_loader)\n",
    "            test_losses.append(test_loss)\n",
    "            \n",
    "            scheduler.step(test_loss)\n",
    "            \n",
    "            if test_loss < best_test_loss:\n",
    "                best_test_loss = test_loss\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'param_count': param_count,\n",
    "            'best_test_loss': best_test_loss,\n",
    "            'final_train_loss': train_losses[-1],\n",
    "            'train_losses': train_losses,\n",
    "            'test_losses': test_losses\n",
    "        }\n",
    "    \n",
    "    def grid_search(self, config_ranges, epochs=80, lr=0.008):\n",
    "        \"\"\"그리드 서치 실행\"\"\"\n",
    "        \n",
    "        # 모든 조합 생성\n",
    "        combinations = list(itertools.product(\n",
    "            config_ranges['hidden_size'],\n",
    "            config_ranges['num_layers'],\n",
    "            config_ranges['dropout'],\n",
    "            config_ranges['batch_size']\n",
    "        ))\n",
    "        \n",
    "        total_combinations = len(combinations)\n",
    "        print(f\"총 {total_combinations}개의 조합을 테스트합니다.\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i, (hidden_size, num_layers, dropout, batch_size) in enumerate(combinations):\n",
    "            print(f\"\\n[{i+1}/{total_combinations}] 테스트 중...\")\n",
    "            print(f\"Hidden Size: {hidden_size}, Layers: {num_layers}, Dropout: {dropout}, Batch Size: {batch_size}\")\n",
    "            \n",
    "            config_start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                result = self.train_single_config(\n",
    "                    hidden_size=hidden_size,\n",
    "                    num_layers=num_layers,\n",
    "                    dropout=dropout,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    lr=lr\n",
    "                )\n",
    "                \n",
    "                config_time = time.time() - config_start_time\n",
    "                \n",
    "                # 결과 저장\n",
    "                config_result = {\n",
    "                    'config_id': i + 1,\n",
    "                    'hidden_size': hidden_size,\n",
    "                    'num_layers': num_layers,\n",
    "                    'dropout': dropout,\n",
    "                    'batch_size': batch_size,\n",
    "                    'param_count': result['param_count'],\n",
    "                    'best_test_loss': result['best_test_loss'],\n",
    "                    'final_train_loss': result['final_train_loss'],\n",
    "                    'training_time': config_time,\n",
    "                    'model_state_dict': result['model'].state_dict(),\n",
    "                    'train_losses': result['train_losses'],\n",
    "                    'test_losses': result['test_losses']\n",
    "                }\n",
    "                \n",
    "                self.results.append(config_result)\n",
    "                \n",
    "                # 모델 저장\n",
    "                model_filename = f\"model_h{hidden_size}_l{num_layers}_d{dropout:.1f}_b{batch_size}.pth\"\n",
    "                torch.save({\n",
    "                    'model_state_dict': result['model'].state_dict(),\n",
    "                    'config': config_result,\n",
    "                    'model_architecture': {\n",
    "                        'hidden_size': hidden_size,\n",
    "                        'num_layers': num_layers,\n",
    "                        'dropout': dropout\n",
    "                    }\n",
    "                }, model_filename)\n",
    "                \n",
    "                print(f\"✓ 완료 - Test Loss: {result['best_test_loss']:.6f}, 시간: {config_time:.1f}초\")\n",
    "                print(f\"  모델 저장: {model_filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"✗ 오류 발생: {str(e)}\")\n",
    "                continue\n",
    "            \n",
    "            # 진행률 및 예상 시간\n",
    "            elapsed_time = time.time() - start_time\n",
    "            avg_time_per_config = elapsed_time / (i + 1)\n",
    "            remaining_configs = total_combinations - (i + 1)\n",
    "            estimated_remaining_time = avg_time_per_config * remaining_configs\n",
    "            \n",
    "            print(f\"진행률: {(i+1)/total_combinations*100:.1f}% | \"\n",
    "                  f\"예상 남은 시간: {estimated_remaining_time/60:.1f}분\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\n그리드 서치 완료! 총 시간: {total_time/3600:.2f}시간\")\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def save_results_table(self, filename=\"grid_search_results.csv\"):\n",
    "        \"\"\"결과를 CSV 테이블로 저장\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"저장할 결과가 없습니다.\")\n",
    "            return\n",
    "        \n",
    "        # DataFrame 생성용 데이터\n",
    "        table_data = []\n",
    "        for result in self.results:\n",
    "            table_data.append({\n",
    "                'Config_ID': result['config_id'],\n",
    "                'Hidden_Size': result['hidden_size'],\n",
    "                'Num_Layers': result['num_layers'],\n",
    "                'Dropout': result['dropout'],\n",
    "                'Batch_Size': result['batch_size'],\n",
    "                'Parameters': result['param_count'],\n",
    "                'Best_Test_Loss': result['best_test_loss'],\n",
    "                'Final_Train_Loss': result['final_train_loss'],\n",
    "                'Overfitting_Gap': result['final_train_loss'] - result['best_test_loss'],\n",
    "                'Training_Time_Min': result['training_time'] / 60\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(table_data)\n",
    "        \n",
    "        # 성능 순으로 정렬\n",
    "        df = df.sort_values('Best_Test_Loss')\n",
    "        df = df.reset_index(drop=True)\n",
    "        df['Rank'] = range(1, len(df) + 1)\n",
    "        \n",
    "        # CSV 저장\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"결과 테이블 저장: {filename}\")\n",
    "        \n",
    "        # 상위 10개 결과 출력\n",
    "        print(\"\\n=== 상위 10개 결과 ===\")\n",
    "        print(df.head(10).to_string(index=False))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def plot_results_analysis(self):\n",
    "        \"\"\"결과 분석 시각화\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"분석할 결과가 없습니다.\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        # 데이터 준비\n",
    "        hidden_sizes = [r['hidden_size'] for r in self.results]\n",
    "        num_layers = [r['num_layers'] for r in self.results]\n",
    "        dropouts = [r['dropout'] for r in self.results]\n",
    "        batch_sizes = [r['batch_size'] for r in self.results]\n",
    "        test_losses = [r['best_test_loss'] for r in self.results]\n",
    "        param_counts = [r['param_count'] for r in self.results]\n",
    "        \n",
    "        # 1. Hidden Size vs Test Loss\n",
    "        axes[0,0].scatter(hidden_sizes, test_losses, alpha=0.7)\n",
    "        axes[0,0].set_xlabel('Hidden Size')\n",
    "        axes[0,0].set_ylabel('Best Test Loss')\n",
    "        axes[0,0].set_title('Hidden Size vs Performance')\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Num Layers vs Test Loss\n",
    "        axes[0,1].scatter(num_layers, test_losses, alpha=0.7)\n",
    "        axes[0,1].set_xlabel('Number of Layers')\n",
    "        axes[0,1].set_ylabel('Best Test Loss')\n",
    "        axes[0,1].set_title('Network Depth vs Performance')\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Dropout vs Test Loss\n",
    "        axes[0,2].scatter(dropouts, test_losses, alpha=0.7)\n",
    "        axes[0,2].set_xlabel('Dropout Rate')\n",
    "        axes[0,2].set_ylabel('Best Test Loss')\n",
    "        axes[0,2].set_title('Dropout Rate vs Performance')\n",
    "        axes[0,2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Batch Size vs Test Loss\n",
    "        axes[1,0].scatter(batch_sizes, test_losses, alpha=0.7)\n",
    "        axes[1,0].set_xlabel('Batch Size')\n",
    "        axes[1,0].set_ylabel('Best Test Loss')\n",
    "        axes[1,0].set_title('Batch Size vs Performance')\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 5. Parameters vs Test Loss\n",
    "        axes[1,1].scatter(param_counts, test_losses, alpha=0.7)\n",
    "        axes[1,1].set_xlabel('Number of Parameters')\n",
    "        axes[1,1].set_ylabel('Best Test Loss')\n",
    "        axes[1,1].set_title('Model Size vs Performance')\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 6. Best configurations 히트맵\n",
    "        best_configs = sorted(self.results, key=lambda x: x['best_test_loss'])[:10]\n",
    "        config_names = [f\"H{c['hidden_size']}_L{c['num_layers']}_D{c['dropout']:.1f}_B{c['batch_size']}\" \n",
    "                       for c in best_configs]\n",
    "        losses = [c['best_test_loss'] for c in best_configs]\n",
    "        \n",
    "        axes[1,2].barh(range(len(config_names)), losses)\n",
    "        axes[1,2].set_yticks(range(len(config_names)))\n",
    "        axes[1,2].set_yticklabels(config_names, fontsize=8)\n",
    "        axes[1,2].set_xlabel('Best Test Loss')\n",
    "        axes[1,2].set_title('Top 10 Configurations')\n",
    "        axes[1,2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('grid_search_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "def main():\n",
    "    # 설정\n",
    "    data_file = \"polynomial_dataset_sorted.json\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"사용 디바이스: {device}\")\n",
    "    \n",
    "    if not os.path.exists(data_file):\n",
    "        print(f\"데이터 파일을 찾을 수 없습니다: {data_file}\")\n",
    "        return\n",
    "    \n",
    "    # 그리드 서치 범위 정의\n",
    "    # 128*4, 128*5, 128*6, 128*7, 128*8, 128*9, 128*10, 128*11, 128*12\n",
    "    config_ranges = {\n",
    "        'hidden_size': [128*3, 128*4, 128*5, 128*6, 128*7],\n",
    "        'num_layers': [6, 7, 8],\n",
    "        'dropout': [0.1],\n",
    "        'batch_size': [64, 128, 64*3, 64*4, 64*5]\n",
    "    }\n",
    "    \n",
    "    print(\"그리드 서치 범위:\")\n",
    "    for key, values in config_ranges.items():\n",
    "        print(f\"  {key}: {values}\")\n",
    "    \n",
    "    total_combinations = 1\n",
    "    for values in config_ranges.values():\n",
    "        total_combinations *= len(values)\n",
    "    print(f\"총 조합 수: {total_combinations}\")\n",
    "    \n",
    "    # 그리드 서치 실행\n",
    "    trainer = GridSearchTrainer(data_file, device)\n",
    "    results = trainer.grid_search(config_ranges, epochs=80, lr=0.008)\n",
    "    \n",
    "    # 결과 저장 및 분석\n",
    "    results_df = trainer.save_results_table(\"polynomial_grid_search_results.csv\")\n",
    "    trainer.plot_results_analysis()\n",
    "    \n",
    "    # 최고 성능 모델 정보\n",
    "    if results:\n",
    "        best_result = min(results, key=lambda x: x['best_test_loss'])\n",
    "        print(f\"\\n🏆 최고 성능 모델:\")\n",
    "        print(f\"  Hidden Size: {best_result['hidden_size']}\")\n",
    "        print(f\"  Num Layers: {best_result['num_layers']}\")\n",
    "        print(f\"  Dropout: {best_result['dropout']}\")\n",
    "        print(f\"  Batch Size: {best_result['batch_size']}\")\n",
    "        print(f\"  Best Test Loss: {best_result['best_test_loss']:.6f}\")\n",
    "        print(f\"  Parameters: {best_result['param_count']:,}\")\n",
    "    \n",
    "    print(\"\\n그리드 서치 완료! 모든 결과가 저장되었습니다. 🎯\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6a5dc2-2a38-495d-979b-0b7fbf9a42e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
