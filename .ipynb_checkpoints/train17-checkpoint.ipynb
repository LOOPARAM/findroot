{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a237bab-8198-457a-953b-a63a7b32943d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‚¬ìš© ë””ë°”ì´ìŠ¤: cuda\n",
      "ê·¸ë¦¬ë“œ ì„œì¹˜ ë²”ìœ„:\n",
      "  hidden_size: [384, 512, 640, 768, 896]\n",
      "  num_layers: [6, 7, 8]\n",
      "  dropout: [0.1]\n",
      "  batch_size: [64, 128, 192, 256, 320]\n",
      "ì´ ì¡°í•© ìˆ˜: 75\n",
      "ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...\n",
      "í›ˆë ¨ ë°ì´í„°ì…‹: 8000ê°œ\n",
      "ë°ì´í„° ì •ê·œí™” ì ìš©ë¨\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹: 2000ê°œ\n",
      "ë°ì´í„° ì •ê·œí™” ì ìš©ë¨\n",
      "ì´ 75ê°œì˜ ì¡°í•©ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.\n",
      "================================================================================\n",
      "\n",
      "[1/75] í…ŒìŠ¤íŠ¸ ì¤‘...\n",
      "Hidden Size: 384, Layers: 6, Dropout: 0.1, Batch Size: 64\n",
      "âœ“ ì™„ë£Œ - Test Loss: 0.155902, ì‹œê°„: 44.4ì´ˆ\n",
      "  ëª¨ë¸ ì €ì¥: model_h384_l6_d0.1_b64.pth\n",
      "ì§„í–‰ë¥ : 1.3% | ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: 54.8ë¶„\n",
      "\n",
      "[2/75] í…ŒìŠ¤íŠ¸ ì¤‘...\n",
      "Hidden Size: 384, Layers: 6, Dropout: 0.1, Batch Size: 128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "\n",
    "class PolynomialDataset(Dataset):\n",
    "    def __init__(self, data_file, train=True, train_ratio=0.8, normalize=True):\n",
    "        \"\"\"\n",
    "        ì €ì¥ëœ ë°ì´í„° íŒŒì¼ë¡œë¶€í„° Dataset ìƒì„±\n",
    "        \n",
    "        Args:\n",
    "            data_file: ë°ì´í„° íŒŒì¼ ê²½ë¡œ (.json ë˜ëŠ” .pkl)\n",
    "            train: Trueë©´ í›ˆë ¨ìš©, Falseë©´ í…ŒìŠ¤íŠ¸ìš©\n",
    "            train_ratio: í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í•  ë¹„ìœ¨\n",
    "            normalize: ë°ì´í„° ì •ê·œí™” ì—¬ë¶€\n",
    "        \"\"\"\n",
    "        self.data = self.load_data(data_file)\n",
    "        self.normalize = normalize\n",
    "        self.coeffs, self.roots = self.prepare_data()\n",
    "        \n",
    "        # ì •ê·œí™” íŒŒë¼ë¯¸í„° ì €ì¥\n",
    "        if normalize:\n",
    "            self.coeff_mean = np.mean(self.coeffs, axis=0)\n",
    "            self.coeff_std = np.std(self.coeffs, axis=0) + 1e-8\n",
    "            self.root_mean = np.mean(self.roots, axis=0)\n",
    "            self.root_std = np.std(self.roots, axis=0) + 1e-8\n",
    "            \n",
    "            # ì •ê·œí™” ì ìš©\n",
    "            self.coeffs = (self.coeffs - self.coeff_mean) / self.coeff_std\n",
    "            self.roots = (self.roots - self.root_mean) / self.root_std\n",
    "        \n",
    "        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• \n",
    "        train_coeffs, test_coeffs, train_roots, test_roots = train_test_split(\n",
    "            self.coeffs, self.roots, train_size=train_ratio, random_state=42\n",
    "        )\n",
    "        \n",
    "        if train:\n",
    "            self.coeffs = train_coeffs\n",
    "            self.roots = train_roots\n",
    "        else:\n",
    "            self.coeffs = test_coeffs\n",
    "            self.roots = test_roots\n",
    "        \n",
    "        print(f\"{'í›ˆë ¨' if train else 'í…ŒìŠ¤íŠ¸'} ë°ì´í„°ì…‹: {len(self.coeffs)}ê°œ\")\n",
    "        if normalize:\n",
    "            print(f\"ë°ì´í„° ì •ê·œí™” ì ìš©ë¨\")\n",
    "    \n",
    "    def load_data(self, filename):\n",
    "        \"\"\"ë°ì´í„° íŒŒì¼ ë¡œë“œ\"\"\"\n",
    "        if filename.endswith('.json'):\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        elif filename.endswith('.pkl'):\n",
    "            with open(filename, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        else:\n",
    "            raise ValueError(\"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. (.json ë˜ëŠ” .pkl ì‚¬ìš©)\")\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        \"\"\"ë°ì´í„°ë¥¼ í•™ìŠµìš© í˜•íƒœë¡œ ë³€í™˜\"\"\"\n",
    "        coeffs = []\n",
    "        roots = []\n",
    "        \n",
    "        for item in self.data:\n",
    "            coeffs.append(item['coefficients'])\n",
    "            \n",
    "            # ê·¼ ë°ì´í„° í˜•ì‹ í™•ì¸ ë° ë³€í™˜\n",
    "            item_roots = item['roots']\n",
    "            \n",
    "            if isinstance(item_roots[0], list):\n",
    "                # [[ì‹¤ìˆ˜ë¶€, í—ˆìˆ˜ë¶€], ...] í˜•íƒœë¥¼ [ì‹¤ìˆ˜ë¶€1, í—ˆìˆ˜ë¶€1, ...] í˜•íƒœë¡œ í‰íƒ„í™”\n",
    "                flattened_roots = []\n",
    "                for root in item_roots:\n",
    "                    flattened_roots.extend(root)\n",
    "                roots.append(flattened_roots)\n",
    "            else:\n",
    "                # ì´ë¯¸ [ì‹¤ìˆ˜ë¶€1, í—ˆìˆ˜ë¶€1, ...] í˜•íƒœ\n",
    "                roots.append(item_roots)\n",
    "        \n",
    "        return np.array(coeffs, dtype=np.float32), np.array(roots, dtype=np.float32)\n",
    "    \n",
    "    def denormalize_roots(self, normalized_roots):\n",
    "        \"\"\"ê·¼ì„ ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë˜ëŒë¦¬ê¸°\"\"\"\n",
    "        if self.normalize and hasattr(self, 'root_mean'):\n",
    "            return normalized_roots * self.root_std + self.root_mean\n",
    "        return normalized_roots\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.coeffs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.FloatTensor(self.coeffs[idx]), torch.FloatTensor(self.roots[idx])\n",
    "\n",
    "class OptimizedPolynomialRootNet(nn.Module):\n",
    "    \"\"\"ê²½ëŸ‰í™”ëœ íš¨ìœ¨ì ì¸ ë‹¤í•­ì‹ ê·¼ ì°¾ê¸° ë„¤íŠ¸ì›Œí¬\"\"\"\n",
    "    def __init__(self, input_size=6, output_size=10, hidden_size=512, num_layers=4, dropout=0.3):\n",
    "        super(OptimizedPolynomialRootNet, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        current_size = input_size\n",
    "        \n",
    "        # ì²« ë²ˆì§¸ ì¸µ\n",
    "        layers.extend([\n",
    "            nn.Linear(current_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Dropout(dropout)\n",
    "        ])\n",
    "        current_size = hidden_size\n",
    "        \n",
    "        # ì¤‘ê°„ ì¸µë“¤ (ì ì§„ì  í¬ê¸° ê°ì†Œ)\n",
    "        for i in range(num_layers - 2):\n",
    "            next_size = hidden_size // (2 ** (i + 1))\n",
    "            next_size = max(next_size, 64)  # ìµœì†Œ 64ê°œ ë‰´ëŸ°\n",
    "            \n",
    "            layers.extend([\n",
    "                nn.Linear(current_size, next_size),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(next_size),\n",
    "                nn.Dropout(dropout * 0.8)  # ì ì§„ì ìœ¼ë¡œ ë“œë¡­ì•„ì›ƒ ê°ì†Œ\n",
    "            ])\n",
    "            current_size = next_size\n",
    "        \n",
    "        # ì¶œë ¥ì¸µ\n",
    "        layers.append(nn.Linear(current_size, output_size))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class GridSearchTrainer:\n",
    "    def __init__(self, data_file, device='cpu'):\n",
    "        self.data_file = data_file\n",
    "        self.device = device\n",
    "        self.results = []\n",
    "        \n",
    "        # ë°ì´í„°ì…‹ í•œ ë²ˆë§Œ ë¡œë“œ (ë§¤ë²ˆ ìƒˆë¡œ ë¡œë“œí•˜ë©´ ì‹œê°„ ì†Œìš”)\n",
    "        print(\"ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...\")\n",
    "        self.train_dataset = PolynomialDataset(data_file, train=True, normalize=True)\n",
    "        self.test_dataset = PolynomialDataset(data_file, train=False, normalize=True)\n",
    "        \n",
    "    def complex_aware_loss(self, pred_roots, true_roots, alpha=0.3):\n",
    "        \"\"\"ë³µì†Œìˆ˜ ê·¼ì„ ê³ ë ¤í•œ ì†ì‹¤ í•¨ìˆ˜\"\"\"\n",
    "        mse_loss = nn.MSELoss()(pred_roots, true_roots)\n",
    "        \n",
    "        pred_real = pred_roots[:, 0::2]\n",
    "        pred_imag = pred_roots[:, 1::2]\n",
    "        true_real = true_roots[:, 0::2]\n",
    "        true_imag = true_roots[:, 1::2]\n",
    "        \n",
    "        pred_magnitude = torch.sqrt(pred_real**2 + pred_imag**2)\n",
    "        true_magnitude = torch.sqrt(true_real**2 + true_imag**2)\n",
    "        magnitude_loss = nn.MSELoss()(pred_magnitude, true_magnitude)\n",
    "        \n",
    "        return (1 - alpha) * mse_loss + alpha * magnitude_loss\n",
    "    \n",
    "    def train_single_config(self, hidden_size, num_layers, dropout, batch_size, epochs=80, lr=0.008):\n",
    "        \"\"\"ë‹¨ì¼ êµ¬ì„±ìœ¼ë¡œ ëª¨ë¸ í›ˆë ¨\"\"\"\n",
    "        \n",
    "        # ë°ì´í„° ë¡œë” ìƒì„±\n",
    "        train_loader = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        test_loader = DataLoader(self.test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "        \n",
    "        # ëª¨ë¸ ìƒì„±\n",
    "        model = OptimizedPolynomialRootNet(\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        ).to(self.device)\n",
    "        \n",
    "        param_count = sum(p.numel() for p in model.parameters())\n",
    "        \n",
    "        # ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=10\n",
    "        )\n",
    "        \n",
    "        # í›ˆë ¨ ê¸°ë¡\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        best_test_loss = float('inf')\n",
    "        \n",
    "        # í›ˆë ¨ ë£¨í”„\n",
    "        for epoch in range(epochs):\n",
    "            # í›ˆë ¨\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for coeffs, roots in train_loader:\n",
    "                coeffs, roots = coeffs.to(self.device), roots.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                pred_roots = model(coeffs)\n",
    "                loss = self.complex_aware_loss(pred_roots, roots)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            train_losses.append(train_loss)\n",
    "            \n",
    "            # í…ŒìŠ¤íŠ¸\n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for coeffs, roots in test_loader:\n",
    "                    coeffs, roots = coeffs.to(self.device), roots.to(self.device)\n",
    "                    pred_roots = model(coeffs)\n",
    "                    loss = self.complex_aware_loss(pred_roots, roots)\n",
    "                    test_loss += loss.item()\n",
    "            \n",
    "            test_loss /= len(test_loader)\n",
    "            test_losses.append(test_loss)\n",
    "            \n",
    "            scheduler.step(test_loss)\n",
    "            \n",
    "            if test_loss < best_test_loss:\n",
    "                best_test_loss = test_loss\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'param_count': param_count,\n",
    "            'best_test_loss': best_test_loss,\n",
    "            'final_train_loss': train_losses[-1],\n",
    "            'train_losses': train_losses,\n",
    "            'test_losses': test_losses\n",
    "        }\n",
    "    \n",
    "    def grid_search(self, config_ranges, epochs=80, lr=0.008):\n",
    "        \"\"\"ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹¤í–‰\"\"\"\n",
    "        \n",
    "        # ëª¨ë“  ì¡°í•© ìƒì„±\n",
    "        combinations = list(itertools.product(\n",
    "            config_ranges['hidden_size'],\n",
    "            config_ranges['num_layers'],\n",
    "            config_ranges['dropout'],\n",
    "            config_ranges['batch_size']\n",
    "        ))\n",
    "        \n",
    "        total_combinations = len(combinations)\n",
    "        print(f\"ì´ {total_combinations}ê°œì˜ ì¡°í•©ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i, (hidden_size, num_layers, dropout, batch_size) in enumerate(combinations):\n",
    "            print(f\"\\n[{i+1}/{total_combinations}] í…ŒìŠ¤íŠ¸ ì¤‘...\")\n",
    "            print(f\"Hidden Size: {hidden_size}, Layers: {num_layers}, Dropout: {dropout}, Batch Size: {batch_size}\")\n",
    "            \n",
    "            config_start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                result = self.train_single_config(\n",
    "                    hidden_size=hidden_size,\n",
    "                    num_layers=num_layers,\n",
    "                    dropout=dropout,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    lr=lr\n",
    "                )\n",
    "                \n",
    "                config_time = time.time() - config_start_time\n",
    "                \n",
    "                # ê²°ê³¼ ì €ì¥\n",
    "                config_result = {\n",
    "                    'config_id': i + 1,\n",
    "                    'hidden_size': hidden_size,\n",
    "                    'num_layers': num_layers,\n",
    "                    'dropout': dropout,\n",
    "                    'batch_size': batch_size,\n",
    "                    'param_count': result['param_count'],\n",
    "                    'best_test_loss': result['best_test_loss'],\n",
    "                    'final_train_loss': result['final_train_loss'],\n",
    "                    'training_time': config_time,\n",
    "                    'model_state_dict': result['model'].state_dict(),\n",
    "                    'train_losses': result['train_losses'],\n",
    "                    'test_losses': result['test_losses']\n",
    "                }\n",
    "                \n",
    "                self.results.append(config_result)\n",
    "                \n",
    "                # ëª¨ë¸ ì €ì¥\n",
    "                model_filename = f\"model_h{hidden_size}_l{num_layers}_d{dropout:.1f}_b{batch_size}.pth\"\n",
    "                torch.save({\n",
    "                    'model_state_dict': result['model'].state_dict(),\n",
    "                    'config': config_result,\n",
    "                    'model_architecture': {\n",
    "                        'hidden_size': hidden_size,\n",
    "                        'num_layers': num_layers,\n",
    "                        'dropout': dropout\n",
    "                    }\n",
    "                }, model_filename)\n",
    "                \n",
    "                print(f\"âœ“ ì™„ë£Œ - Test Loss: {result['best_test_loss']:.6f}, ì‹œê°„: {config_time:.1f}ì´ˆ\")\n",
    "                print(f\"  ëª¨ë¸ ì €ì¥: {model_filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âœ— ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "                continue\n",
    "            \n",
    "            # ì§„í–‰ë¥  ë° ì˜ˆìƒ ì‹œê°„\n",
    "            elapsed_time = time.time() - start_time\n",
    "            avg_time_per_config = elapsed_time / (i + 1)\n",
    "            remaining_configs = total_combinations - (i + 1)\n",
    "            estimated_remaining_time = avg_time_per_config * remaining_configs\n",
    "            \n",
    "            print(f\"ì§„í–‰ë¥ : {(i+1)/total_combinations*100:.1f}% | \"\n",
    "                  f\"ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {estimated_remaining_time/60:.1f}ë¶„\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nê·¸ë¦¬ë“œ ì„œì¹˜ ì™„ë£Œ! ì´ ì‹œê°„: {total_time/3600:.2f}ì‹œê°„\")\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def save_results_table(self, filename=\"grid_search_results.csv\"):\n",
    "        \"\"\"ê²°ê³¼ë¥¼ CSV í…Œì´ë¸”ë¡œ ì €ì¥\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return\n",
    "        \n",
    "        # DataFrame ìƒì„±ìš© ë°ì´í„°\n",
    "        table_data = []\n",
    "        for result in self.results:\n",
    "            table_data.append({\n",
    "                'Config_ID': result['config_id'],\n",
    "                'Hidden_Size': result['hidden_size'],\n",
    "                'Num_Layers': result['num_layers'],\n",
    "                'Dropout': result['dropout'],\n",
    "                'Batch_Size': result['batch_size'],\n",
    "                'Parameters': result['param_count'],\n",
    "                'Best_Test_Loss': result['best_test_loss'],\n",
    "                'Final_Train_Loss': result['final_train_loss'],\n",
    "                'Overfitting_Gap': result['final_train_loss'] - result['best_test_loss'],\n",
    "                'Training_Time_Min': result['training_time'] / 60\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(table_data)\n",
    "        \n",
    "        # ì„±ëŠ¥ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "        df = df.sort_values('Best_Test_Loss')\n",
    "        df = df.reset_index(drop=True)\n",
    "        df['Rank'] = range(1, len(df) + 1)\n",
    "        \n",
    "        # CSV ì €ì¥\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"ê²°ê³¼ í…Œì´ë¸” ì €ì¥: {filename}\")\n",
    "        \n",
    "        # ìƒìœ„ 10ê°œ ê²°ê³¼ ì¶œë ¥\n",
    "        print(\"\\n=== ìƒìœ„ 10ê°œ ê²°ê³¼ ===\")\n",
    "        print(df.head(10).to_string(index=False))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def plot_results_analysis(self):\n",
    "        \"\"\"ê²°ê³¼ ë¶„ì„ ì‹œê°í™”\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"ë¶„ì„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        # ë°ì´í„° ì¤€ë¹„\n",
    "        hidden_sizes = [r['hidden_size'] for r in self.results]\n",
    "        num_layers = [r['num_layers'] for r in self.results]\n",
    "        dropouts = [r['dropout'] for r in self.results]\n",
    "        batch_sizes = [r['batch_size'] for r in self.results]\n",
    "        test_losses = [r['best_test_loss'] for r in self.results]\n",
    "        param_counts = [r['param_count'] for r in self.results]\n",
    "        \n",
    "        # 1. Hidden Size vs Test Loss\n",
    "        axes[0,0].scatter(hidden_sizes, test_losses, alpha=0.7)\n",
    "        axes[0,0].set_xlabel('Hidden Size')\n",
    "        axes[0,0].set_ylabel('Best Test Loss')\n",
    "        axes[0,0].set_title('Hidden Size vs Performance')\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Num Layers vs Test Loss\n",
    "        axes[0,1].scatter(num_layers, test_losses, alpha=0.7)\n",
    "        axes[0,1].set_xlabel('Number of Layers')\n",
    "        axes[0,1].set_ylabel('Best Test Loss')\n",
    "        axes[0,1].set_title('Network Depth vs Performance')\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Dropout vs Test Loss\n",
    "        axes[0,2].scatter(dropouts, test_losses, alpha=0.7)\n",
    "        axes[0,2].set_xlabel('Dropout Rate')\n",
    "        axes[0,2].set_ylabel('Best Test Loss')\n",
    "        axes[0,2].set_title('Dropout Rate vs Performance')\n",
    "        axes[0,2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Batch Size vs Test Loss\n",
    "        axes[1,0].scatter(batch_sizes, test_losses, alpha=0.7)\n",
    "        axes[1,0].set_xlabel('Batch Size')\n",
    "        axes[1,0].set_ylabel('Best Test Loss')\n",
    "        axes[1,0].set_title('Batch Size vs Performance')\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 5. Parameters vs Test Loss\n",
    "        axes[1,1].scatter(param_counts, test_losses, alpha=0.7)\n",
    "        axes[1,1].set_xlabel('Number of Parameters')\n",
    "        axes[1,1].set_ylabel('Best Test Loss')\n",
    "        axes[1,1].set_title('Model Size vs Performance')\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 6. Best configurations íˆíŠ¸ë§µ\n",
    "        best_configs = sorted(self.results, key=lambda x: x['best_test_loss'])[:10]\n",
    "        config_names = [f\"H{c['hidden_size']}_L{c['num_layers']}_D{c['dropout']:.1f}_B{c['batch_size']}\" \n",
    "                       for c in best_configs]\n",
    "        losses = [c['best_test_loss'] for c in best_configs]\n",
    "        \n",
    "        axes[1,2].barh(range(len(config_names)), losses)\n",
    "        axes[1,2].set_yticks(range(len(config_names)))\n",
    "        axes[1,2].set_yticklabels(config_names, fontsize=8)\n",
    "        axes[1,2].set_xlabel('Best Test Loss')\n",
    "        axes[1,2].set_title('Top 10 Configurations')\n",
    "        axes[1,2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('grid_search_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "def main():\n",
    "    # ì„¤ì •\n",
    "    data_file = \"polynomial_dataset_sorted.json\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
    "    \n",
    "    if not os.path.exists(data_file):\n",
    "        print(f\"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_file}\")\n",
    "        return\n",
    "    \n",
    "    # ê·¸ë¦¬ë“œ ì„œì¹˜ ë²”ìœ„ ì •ì˜\n",
    "    # 128*4, 128*5, 128*6, 128*7, 128*8, 128*9, 128*10, 128*11, 128*12\n",
    "    config_ranges = {\n",
    "        'hidden_size': [128*3, 128*4, 128*5, 128*6, 128*7],\n",
    "        'num_layers': [6, 7, 8],\n",
    "        'dropout': [0.1],\n",
    "        'batch_size': [64, 128, 64*3, 64*4, 64*5]\n",
    "    }\n",
    "    \n",
    "    print(\"ê·¸ë¦¬ë“œ ì„œì¹˜ ë²”ìœ„:\")\n",
    "    for key, values in config_ranges.items():\n",
    "        print(f\"  {key}: {values}\")\n",
    "    \n",
    "    total_combinations = 1\n",
    "    for values in config_ranges.values():\n",
    "        total_combinations *= len(values)\n",
    "    print(f\"ì´ ì¡°í•© ìˆ˜: {total_combinations}\")\n",
    "    \n",
    "    # ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹¤í–‰\n",
    "    trainer = GridSearchTrainer(data_file, device)\n",
    "    results = trainer.grid_search(config_ranges, epochs=80, lr=0.008)\n",
    "    \n",
    "    # ê²°ê³¼ ì €ì¥ ë° ë¶„ì„\n",
    "    results_df = trainer.save_results_table(\"polynomial_grid_search_results.csv\")\n",
    "    trainer.plot_results_analysis()\n",
    "    \n",
    "    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì •ë³´\n",
    "    if results:\n",
    "        best_result = min(results, key=lambda x: x['best_test_loss'])\n",
    "        print(f\"\\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸:\")\n",
    "        print(f\"  Hidden Size: {best_result['hidden_size']}\")\n",
    "        print(f\"  Num Layers: {best_result['num_layers']}\")\n",
    "        print(f\"  Dropout: {best_result['dropout']}\")\n",
    "        print(f\"  Batch Size: {best_result['batch_size']}\")\n",
    "        print(f\"  Best Test Loss: {best_result['best_test_loss']:.6f}\")\n",
    "        print(f\"  Parameters: {best_result['param_count']:,}\")\n",
    "    \n",
    "    print(\"\\nê·¸ë¦¬ë“œ ì„œì¹˜ ì™„ë£Œ! ëª¨ë“  ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ğŸ¯\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6a5dc2-2a38-495d-979b-0b7fbf9a42e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
